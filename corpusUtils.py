"""
Utils to create ML-ready corpora from files generated by munge.py
"""
from __future__ import unicode_literals
from multiprocessing import Manager, Pool
from glob import glob
from collections import Counter
import codecs
import os
import json
from spacy.en import English
tokenize = English(tagger=None,parser=None,entity=None)
import regex as re

ROOT_DIR = os.path.split(os.path.realpath(__file__))[0]
TALK_FILES_EXTRACTED_DIR = os.path.join(ROOT_DIR,'data/talk_pages_structured_json')
g = glob(os.path.join(TALK_FILES_EXTRACTED_DIR,'*.json'))


class MyCorpus(object):
    """
    Class for reading corpus files generated by createCleanCorpus
    """
    def __init__(self,num_authors,start=0,stop=float('inf')):
        self.ROOT_DIR = os.path.split(os.path.realpath(__file__))[0]
        self.corpus_file = os.path.join(self.ROOT_DIR,'data/{}_authors.corpus'.format(num_authors))
        self.start = start # which line to start reading from
        self.stop = stop
    def __len__(self):
        """ Number of lines in corpus """
        l = 0
        for line in codecs.open(self.corpus_file,'r','utf8'):
            l += 1
        return l    
    def __iter__(self):
        "Note: only outputs post text, not the author"
        i = 0
        for line in codecs.open(self.corpus_file,'r','utf8'):
            if i >= self.start and i < self.stop and not line.startswith('>>>USER:') and not line.startswith('\n'):
                yield line
            i += 1

def authorStats(g):
    """ Collect the most prolific authors by number of chars """
    authors = Counter()
    for p in g:
        with open(p) as fi:
            page = json.load(fi)
        if len(page[0]) > 0:
            for topic in page[0]:
                if len(topic) > 0:
                    for post in topic['posts']:
                        author = post['author']
                        text = post['post']
                        authors[author] += len(text)
    authors = sorted([(k,v) for k, v in authors.iteritems()],key=lambda x:x[1],reverse=True) # sort most prolific authors decending
    
    # TODO: actually check if author is a bot against this list: https://en.wikipedia.org/wiki/Wikipedia:Bots/Status
    no_bots = [x for x in authors if 'bot' not in x[0].lower()]    # NOTE: Not all bots have 'bot' in the username
    with open(os.path.join(ROOT_DIR,'data/authors.json'),'wb') as fo:
        json.dump(no_bots,fo)
        
# Some of the below regex were taken from wikifil.pl by Matt Mahoney (http://mattmahoney.net/dc/textdata.html)
title = re.compile(r'^===?[^=]*===?',flags=re.M) # Topic titles
and_ = re.compile(r'&amp;') # decode URL encoded chars
lt = re.compile(r'&lt;')
gt = re.compile(r'&gt;')
ref = re.compile(r'<ref[^<]*<\/ref>') # remove references <ref...> ... </ref>
curly = re.compile(r'\{\{[^\{]*?\}\}') # curly markup brackets
reply = re.compile(r':',flags=re.M)  # ::: reply structure
user = re.compile(r'\[\[User.*?\]\]') # usernames
special = re.compile(r'\[\[Special\:Contributions\/.*?\]\]') # more usernames 
html_tags =  re.compile(r'<[^>]*>') # XHMTL tags
wiki_links = re.compile(r'\[\[[^\|\]]*\|')  # remove wiki url, preserve visible text
brackets = re.compile(r'(\[\[?|\]\]?)') # remove brackets
link = re.compile(r'https?://.*?\s') # links
url_encoded = re.compile(r'&[^;]*;') # remove URL encoded chars
def cleanText(text):
    """ clean the text to be more parsable """
    text = text.strip()
    text = text.replace('\n', ' ')
    text= title.sub('',text)
    text = and_.sub('&',text)
    text = lt.sub('<',text)
    text = gt.sub('>',text)
    text = curly.sub('',text) 
    text = curly.sub('',text) # double nested curly brackets
    text = ref.sub('',text)
    text= reply.sub('',text)
    text= special.sub('',text)
    text= user.sub('**USER**',text)
    text= html_tags.sub('',text)
    text= wiki_links.sub('[[',text)
    text= brackets.sub('',text)
    text = link.sub('**LINK**  ',text)
    text = url_encoded.sub(' ',text)
    text = text.strip()
    return text
        

def createCleanCorpus(g,num_top_authors):
    """
    output corpus file with cleaned post text with only num_top_authors
    author's name immediately precedes text. One post per line
    
    Corpus format (utf-8):
    
    >>>USER:wikipedia_user_name
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor ....
    
    >>>USER:another_user_name_23
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor ...
    
    Output file name: 'data/{}_authors.corpus'.format(num_top_authors)
    """
    cleaned_corpus_file = os.path.join(ROOT_DIR,'data/{}_authors.corpus'.format(num_top_authors))
    with open(os.path.join(ROOT_DIR,'data/authors.json')) as fi:
        authors = json.load(fi)
    authors = authors[:num_top_authors]
    author_list = set(x[0] for x in authors)
    for p in g:
        with open(p) as fi:
            page = json.load(fi)
        if len(page[0]) > 0:
            for topic in page[0]:
                if len(topic) > 0:
                    for post in topic['posts']:
                        author = post['author']
                        if author in author_list:
                            text = post['post']
                            text = cleanText(text)
                            output_text = '>>>USER:{}\n{}\n\n'.format(author,text)
                            with codecs.open(cleaned_corpus_file,'ab','utf-8') as fo:
                                fo.write(output_text)
                            print author


def addToDict(kwargs):
    """
    put each token into a dict with a unique id
    also calculate the longest sentence by number of tokens
    """
    corpus = MyCorpus(kwargs['n_authors'],start=kwargs['start'],stop=kwargs['stop'])
    for doc in corpus:
        for sentence in tokenize(doc).sents:
            sen_len = len(sentence)
            if sen_len > kwargs['longest_sentence'].value:
                kwargs['longest_sentence'].value = sen_len
            for tok in sentence:
                token = tok.orth_.lower()
                if not token in kwargs['d']:
                    kwargs['d'][token] = len(kwargs['d'])

def createVocabDict(n_authors,n_threads):
    """
    Create a vocabulary for the corpus
    
    input: n_authors -- corpus file to read. Looks for 'data/{}_authors.corpus'.format(num_top_authors)
           n_threads -- number of jobs to run. Splits corpus into n_threads to run tokenizer in parallel.
           
    output: vocab -- dict where each word in the corpus lexicon has a unique id
            longest_sentence -- num of tokens in the longest sentence
    """
    len_corpus = len(MyCorpus(n_authors))# read corpus and figure this out
    m=Manager() # shared between processes 
    vocab = m.dict()
    longest_sentence = m.Value('i',0)
    pool = Pool(n_threads)
    args = [{'d':vocab,'start':start,'stop':start+len_corpus/n_threads,'n_authors':n_authors,
                         'longest_sentence':longest_sentence} \
                        for start in range(0,len_corpus,len_corpus/n_threads)]
    args[-1]['stop']  = float('inf') # make sure the last thread reads the end of the corpus 
    pool.map(addToDict,args)
    vocab = dict(vocab)
    longest_sentence = longest_sentence.value
    with open(os.path.join(ROOT_DIR,'data/{}_authors.stats'.format(n_authors)),'wb') as fo:
        json.dump({'longest_sentence':longest_sentence,'vocab_size':len(vocab)},fo)
    with open(os.path.join(ROOT_DIR,'data/vocab_{}_authors.dict'.format(n_authors)),'wb') as fo:
        json.dump(vocab,fo)
    return vocab,longest_sentence,len(vocab)

if __name__ == '__main__':
    n_authors = 50
    createCleanCorpus(g,n_authors)
    createVocabDict(n_authors,8)
